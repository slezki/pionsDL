{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import uproot\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filetype = 'RS'\n",
    "filetype = 'sidebands'\n",
    "#filetype = 'WS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_background_datasetMaker(p1, p2):\n",
    "\n",
    "    p1_variables = [p1 + var for var in pion_variables]\n",
    "    p2_variables = [p2 + var for var in pion_variables]\n",
    "\n",
    "    df_t = uproot.open(root_file)[t_tree] # read tTrue tree\n",
    "    df_f = uproot.open(root_file)[f_tree] # read tFake tree\n",
    "\n",
    "    df1_t = df_t.pandas.df(p1_variables,flatten=False) # base dataframe for True pi1\n",
    "    df1_t.columns = new_cols\n",
    "\n",
    "    df2_t = df_t.pandas.df(p2_variables,flatten=False) # base dataframe for True pi2\n",
    "    df2_t.columns = new_cols\n",
    "\n",
    "    df1_f = df_f.pandas.df(p1_variables,flatten=False) # base dataframe for Fake pi1\n",
    "    df1_f.columns = new_cols\n",
    "\n",
    "    df2_f = df_f.pandas.df(p2_variables,flatten=False) # base dataframe for Fake pi2\n",
    "    df2_f.columns = new_cols\n",
    "\n",
    "    # Now read additional features from TLorentzVector for True pi1 and pi2, and for Fake pi1 and pi2\n",
    "    p1p4 = p1+'_p4'\n",
    "    p2p4 = p2+'_p4'\n",
    "\n",
    "    l1_t = np.array([df_t.array(p1p4).E, df_t.array(p1p4).pt, df_t.array(p1p4).eta, df_t.array(p1p4).phi, df_t.array(p1p4).x, df_t.array(p1p4).y, df_t.array(p1p4).z]).T\n",
    "    l2_t = np.array([df_t.array(p2p4).E, df_t.array(p2p4).pt, df_t.array(p2p4).eta, df_t.array(p2p4).phi, df_t.array(p2p4).x, df_t.array(p2p4).y, df_t.array(p2p4).z]).T\n",
    "\n",
    "    l1_f = np.array([df_f.array(p1p4).E, df_f.array(p1p4).pt, df_f.array(p1p4).eta, df_f.array(p1p4).phi, df_f.array(p1p4).x, df_f.array(p1p4).y, df_f.array(p1p4).z]).T\n",
    "    l2_f = np.array([df_f.array(p2p4).E, df_f.array(p2p4).pt, df_f.array(p2p4).eta, df_f.array(p2p4).phi, df_f.array(p2p4).x, df_f.array(p2p4).y, df_f.array(p2p4).z]).T\n",
    "\n",
    "    # Add the new features to the previous dataframes\n",
    "    df1_t = pd.concat([pd.DataFrame(l1_t, columns=lorentz_var), df1_t], axis=1)\n",
    "    df2_t = pd.concat([pd.DataFrame(l2_t, columns=lorentz_var), df2_t], axis=1)\n",
    "\n",
    "    df1_f = pd.concat([pd.DataFrame(l1_f, columns=lorentz_var), df1_f], axis=1)\n",
    "    df2_f = pd.concat([pd.DataFrame(l2_f, columns=lorentz_var), df2_f], axis=1)\n",
    "\n",
    "    # Read Y1Spp mass\n",
    "    dfy1s_t = df_t.pandas.df('Y1Spipi_M',flatten=False)\n",
    "    dfy1s_f = df_f.pandas.df('Y1Spipi_M',flatten=False)\n",
    "\n",
    "    # Read event number\n",
    "    dfevt_t = df_t.pandas.df('event',flatten=False)\n",
    "    dfevt_f = df_f.pandas.df('event',flatten=False)\n",
    "\n",
    "    # Create ground truths\n",
    "    gt_t = np.ones(len(df1_t))\n",
    "    gt_f = np.zeros(len(df1_f))\n",
    "\n",
    "    # Concatenate the event information, the ground truth and Y1S dataset\n",
    "    df3_t = pd.concat([dfevt_t, dfy1s_t, pd.DataFrame(gt_t, columns=['label'])], axis=1)\n",
    "    df3_f = pd.concat([dfevt_f, dfy1s_f, pd.DataFrame(gt_f, columns=['label'])], axis=1)\n",
    "\n",
    "    # Concatenate the datasets for pi1, pi2 and additional info separately\n",
    "    df1_all = pd.concat([df1_t, df1_f], axis=0)\n",
    "    df2_all = pd.concat([df2_t, df2_f], axis=0)\n",
    "    df3_all = pd.concat([df3_t, df3_f], axis=0)\n",
    "\n",
    "    # Check that the datasets are correct (use only one print)\n",
    "    print(df1_all.head())\n",
    "    print('Len: {}'.format(len(df1_all)))\n",
    "\n",
    "    print(df2_all.head())\n",
    "    print('Len: {}'.format(len(df2_all)))\n",
    "\n",
    "    print(df3_all.head())\n",
    "    print('Len: {}'.format(len(df3_all)))\n",
    "\n",
    "    # Now save datasets in a single hdf5 file\n",
    "    df1_all.to_hdf(dataset_dir + filetype + '.h5', \"pion1\")\n",
    "    df2_all.to_hdf(dataset_dir + filetype + '.h5', \"pion2\")\n",
    "    df3_all.to_hdf(dataset_dir + filetype + '.h5', \"add_info\")\n",
    "\n",
    "    print('Datasets for signal + background correctly saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_background_datasetMaker(p1, p2):\n",
    "\n",
    "    p1_variables = [p1 + var for var in pion_variables]\n",
    "    p2_variables = [p2 + var for var in pion_variables]\n",
    "\n",
    "    df_all = uproot.open(root_file)[a_tree] # read tAll tree\n",
    "\n",
    "    df1_all = df_all.pandas.df(p1_variables,flatten=False) # base dataframe for pi1\n",
    "    df1_all.columns = new_cols\n",
    "\n",
    "    df2_all = df_all.pandas.df(p2_variables,flatten=False) # base dataframe for pi2\n",
    "    df2_all.columns = new_cols\n",
    "\n",
    "    # Now read additional features from TLorentzVector for pi1 and pi2\n",
    "    p1p4 = p1+'_p4'\n",
    "    p2p4 = p2+'_p4'\n",
    "    l1_all = np.array([df_all.array(p1p4).E, df_all.array(p1p4).pt, df_all.array(p1p4).eta, df_all.array(p1p4).phi, df_all.array(p1p4).x, df_all.array(p1p4).y, df_all.array(p1p4).z]).T\n",
    "    l2_all = np.array([df_all.array(p2p4).E, df_all.array(p2p4).pt, df_all.array(p2p4).eta, df_all.array(p2p4).phi, df_all.array(p2p4).x, df_all.array(p2p4).y, df_all.array(p2p4).z]).T\n",
    "\n",
    "    # Add the new features to the previous dataframes\n",
    "    df1_all = pd.concat([pd.DataFrame(l1_all, columns=lorentz_var), df1_all], axis=1)\n",
    "    df2_all = pd.concat([pd.DataFrame(l2_all, columns=lorentz_var), df2_all], axis=1)\n",
    "\n",
    "    # Read Y1Spp mass\n",
    "    dfy1s_all = df_all.pandas.df('Y1Spipi_M',flatten=False)\n",
    "\n",
    "    # Read event number\n",
    "    dfevt_all = df_all.pandas.df('event',flatten=False)\n",
    "\n",
    "    # Create ground truths\n",
    "    gt_all = np.zeros(len(df1_all))\n",
    "\n",
    "    # Concatenate the event information, the ground truth and Y1S dataset\n",
    "    df3_all = pd.concat([dfevt_all, dfy1s_all, pd.DataFrame(gt_all, columns=['label'])], axis=1)\n",
    "\n",
    "    # Check that the datasets are correct (use only one print)\n",
    "    print(df1_all.head())\n",
    "    print('Len: {}'.format(len(df1_all)))\n",
    "\n",
    "    print(df2_all.head())\n",
    "    print('Len: {}'.format(len(df2_all)))\n",
    "\n",
    "    print(df3_all.head())\n",
    "    print('Len: {}'.format(len(df3_all)))\n",
    "\n",
    "    # Now save datasets in a single hdf5 file\n",
    "    df1_all.to_hdf(dataset_dir + filetype + '.h5', \"pion1\")\n",
    "    df2_all.to_hdf(dataset_dir + filetype + '.h5', \"pion2\")\n",
    "    df3_all.to_hdf(dataset_dir + filetype + '.h5', \"add_info\")\n",
    "\n",
    "    print('Datasets for background-only correctly saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          E        pt       eta       phi        px        py        pz  \\\n",
      "0  1.446021  0.973145 -0.943388 -2.600870 -0.834313 -0.500932 -1.060418   \n",
      "1  0.956903  0.670898 -0.878201  3.075003 -0.669412  0.044642 -0.667891   \n",
      "2  0.956903  0.670898 -0.878201  3.075003 -0.669412  0.044642 -0.667891   \n",
      "3  1.202009  1.193359 -0.029481 -1.864128 -0.345051 -1.142386 -0.035186   \n",
      "4  0.815838  0.803223 -0.038270 -1.077955  0.380030 -0.707633 -0.030747   \n",
      "\n",
      "   charge       dxy        dz  dRwithDimuon  fromPV  \n",
      "0     1.0  0.013237  1.419823      0.305293     3.0  \n",
      "1     1.0  0.013741  1.387934      0.309457     2.0  \n",
      "2     1.0  0.013741  1.387934      0.309457     2.0  \n",
      "3     1.0  0.007943 -0.812138      0.324929     2.0  \n",
      "4     1.0  0.008500  6.248265      0.291011     3.0  \n",
      "Len: 631916\n",
      "          E        pt       eta       phi        px        py        pz  \\\n",
      "0  1.985643  1.489258 -0.791589 -2.806720 -1.406533 -0.489444 -1.305913   \n",
      "1  1.985643  1.489258 -0.791589 -2.806720 -1.406533 -0.489444 -1.305913   \n",
      "2  1.284483  0.700195 -1.208533 -2.633193 -0.611638 -0.340841 -1.067775   \n",
      "3  0.810331  0.775391 -0.242073 -1.924773 -0.268774 -0.727318 -0.189540   \n",
      "4  0.672606  0.657227  0.047426 -1.007837  0.350756 -0.555803  0.031181   \n",
      "\n",
      "   charge       dxy        dz  dRwithDimuon  fromPV  \n",
      "0    -1.0  0.007162  1.401937      0.125158     3.0  \n",
      "1    -1.0  0.007162  1.401937      0.125158     3.0  \n",
      "2    -1.0  0.023025  1.380458      0.425206     3.0  \n",
      "3    -1.0  0.008118 -0.793436      0.104225     2.0  \n",
      "4    -1.0  0.018474  6.231667      0.228979     3.0  \n",
      "Len: 631916\n",
      "       event  Y1Spipi_M  label\n",
      "0  663471005  10.052678    0.0\n",
      "1  663471005   9.982776    0.0\n",
      "2  663471005  10.097316    0.0\n",
      "3  663776650  10.055971    0.0\n",
      "4  663825484   9.971722    0.0\n",
      "Len: 631916\n",
      "Datasets for background-only correctly saved!\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../dataset/'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "t_tree = 'tTrue'\n",
    "f_tree = 'tFake'\n",
    "a_tree = 'tAll'\n",
    "\n",
    "pion_variables = ['_charge','_dxy', '_dz', '_dRwithDimuon', '_fromPV'] # base features for pions\n",
    "new_cols = ['charge', 'dxy', 'dz', 'dRwithDimuon', 'fromPV'] # easy rename, since datasets will be separated\n",
    "lorentz_var = ['E', 'pt', 'eta', 'phi', 'px', 'py', 'pz']\n",
    "\n",
    "if(filetype == 'RS'):\n",
    "    root_file = '/lustre/cms/store/user/slezki/Filtered/forDNN/histo_y2s2y1spp_RS_2018MC_v2.root'\n",
    "    p1 = 'pion1'\n",
    "    p2 = 'pion2'\n",
    "    signal_background_datasetMaker(p1, p2)\n",
    "elif(filetype == 'sidebands'):\n",
    "    root_file = '/lustre/cms/store/user/slezki/Filtered/forDNN/histo_xb2y1spp_RS_2018DataRunII_BKGsidebands_v2.root'\n",
    "    p1 = 'pionP'\n",
    "    p2 = 'pionM'\n",
    "    additional_background_datasetMaker(p1,p2)\n",
    "elif(filetype == 'WS'):\n",
    "    root_file = '/lustre/cms/store/user/slezki/Filtered/forDNN/histo_xb2y1spp_WS_2018DataRunII_underY2S_v2.root'\n",
    "    p1 = 'pion1'\n",
    "    p2 = 'pion2'\n",
    "    additional_background_datasetMaker(p1,p2)\n",
    "else:\n",
    "    print('ERROR: filetype not understood')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_charge', '_dxy', '_dz', '_dRwithDimuon', '_fromPV']\n"
     ]
    }
   ],
   "source": [
    "#print(pion_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
